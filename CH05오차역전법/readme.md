오차역전법은 cs231 backpropagation 인강이 최고이다.

<br>

# Affine 변환의 오차역전파

W^T에서 T는 전치행렬을 의미.

> 전치행렬이란, W의 (i, j)위치 원소를 (j, i)위치로 바꾼 것

- np.dot의 미분은 W의 전치행렬이 된다. (왜 이렇게 되는지 증명할 순 없는 수준이지만, 일단 외우고 넘어가자!)
- 전치행렬이 되는 위치도 중요하다! X\*W에서 X에 대한 미분이면 dX * W_T, W에 대한 미분이면 X_T * dW이다!
- 미분 전의 행렬의 shape과 미분 후의 행렬 shape은 동일해야 한다!

<br>

## batch용 Affine 변환
batch가 추가된것 말고 다른 것은 없음

# softmax with loss 층

- 신경망에선 **학습**과 **추론**을 수행합니다. softmax는 학습에선 이용하지만, 추론에선 이용하지 않는다고 합니다. 추론에서 답을 낼 때 필요한 것은, `어떤 값이 가장 높은가`이기 때문에 정규화가 중요하지 않습니다. 그렇다면, 학습에서는 softmax가 필요한 이유가 궁금해졌습니다. 개인적으론 backprob을 할 때, 너무 큰 값이 전파되지 않도록 하기 위해서 인것 같은데, 자료를 찾아보니 다음과 같습니다.


https://velog.io/@francomoon7/%EC%98%88%EC%B8%A1%EC%97%90-Softmax%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EB%A9%B4-%EC%95%88%EB%90%98%EB%8A%94-%EC%9D%B4%EC%9C%A0

여기 내용 정리하기!
