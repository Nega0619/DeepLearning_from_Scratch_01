# 신경망

- 신경망을 표현하는 2가지 방법
<br>
입력 - 은닉층 1층 - 출력층으로 이루어진 신경망이 있을 때 <br>
1. 신경망의 층을 기준으로 3층 신경망이라고 표현<br>
2. 가중치 층이 2층이므로 2층 신경망이라고 표현

<br>

## bias와 활성화 함수가 적용된 신경망 구조도

![image](https://user-images.githubusercontent.com/80737049/176901108-74dd8ed7-9996-45f3-8958-65c4d56cf637.png)

h() : 활성화함수. 가중치와 입력데이터의 곱을 다음 노드로 넘겨줄지 아닐지를 결정하는 임계치의 기준을 가지고 있음.

> 명심할 점
> 
> 은닉층 노드안에 `입력데이터*가중치 합`노드와 출력층 노드로 이루어져 있으며, 이 두 노드는 활성화 함수 activation function으로 연결되어 있다.
> 

## 활성화 함수 종류

은닉층에 사용하는 경우와 출력층에 사용하는 활성화 함수가 다르다.

일반적으로 회귀에는 항등함수를, 분류에는 소프트 맥스를 사용한다.

은닉층에 사용하는 경우 : sigmoid, relu, leaky relu 등

출력층에 사용하는 경우 : softmax, identity function(항등 함수)

<br>

### sigmoid

<br>

![image](https://user-images.githubusercontent.com/80737049/176901152-89be754e-0a1c-41c4-ab4f-c2ae0d9af2a7.png)

### relu

<br>

### leakyRelu

<br>

### tanh

<br>

### softmax 

![image](https://user-images.githubusercontent.com/80737049/176914224-c60e87a2-8428-4473-b595-06335c8c712b.png)

위 수식을 그대로 구현하면 발생할 수 있는 문제 : 오버플로

이를 방지하기 위해, 임의의 숫자 C를 분자, 분모에 곱해주고, 

C를 logC로 바꾸어 exp(a_k + logC) 형식으로 바꿔준다.

> 내 생각엔 lnC가 더 정확한 것 같다.

어쨌든, logC -> C'이라는 문자로 치환하면, exp(a_k + C') / sigma(exp(a_i~k + C'))로 수정이 가능하다.

어떤 C값이든 상관없지만, 오버플로우를 막을 목적으로 C는 보통 입력신호 중 최대값을 이용하는것이 일반적 : -max(입력신호)

> 기억!
> 
> 학습시킬 때는 소프트맥스층을 사용하지만,
> test할 때는 소프트 맥스를 사용하지 않는 것이 일반적이다. 
> 
> -> exp는 상등함수이기 때문에 출력값의 대소를 변화시키지 않는다.
> 
> -> 신경망을 이용한 분류에서  가장 큰 출력을 내는 뉴런을 클래스로 인식
> 
> 위 두 이유로, test, 추론, 단계에서는 지수함수를 계산하는 자원낭비를 막고자 생략합니다.

<br>

## 신경망 연산

### 구현하는 방법 : 다차원 배열끼리의 곱

#### 넘파이 브로드캐스트 기능

<br>

넘파이 배열과 스칼라 값의 연산을 넘파이 배열의 원소 각각과 스칼라 값의 연산으로 바꿔 수행하는 것

#### 배열의 크기뽑기

<br>

A = np.array([1, 2, 3, 4])일 때 <br>

- np.ndim(A) : 1 : 배열의 차원 수
- A.shape : (4,) : 배열의 형상, 튜플형식
  - (4,) : [1, 2, 3, 4]
  - (4,1): [ [1, 2, 3, 4] ]
- A.shape[0] : 4

### 다차원 행렬 곱

numpy.dot으료 연산 한다.
- (행, 열), (열,)인 경우
- (행, 열), (열, 행')인 경우
- (행,), (행, 열)인 경우

위 경우에 가능하다.

## 3층 신경망

init_network 함수와 forward 함수가 존재.
